{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"BSTCalcMethods.ipynb","provenance":[],"mount_file_id":"1KcGlATZjdhsnnMkmhVzArGtjTpRSuWIh","authorship_tag":"ABX9TyPE4NHi4bqDOQY7B3qCnt+B"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"4tbFsBRPA301"},"outputs":[],"source":["#!/opt/local/bin python\n","#import sys\n","#sys.path.reverse()\n","\n","    #   Earthquake Methods library of methods and functions\n","    #   \n","    #   This code base collects the methods and functions used to make\n","    #   plots and maps of earthquake data and activity\n","    #\n","    ######################################################################"]},{"cell_type":"code","source":["import sys\n","import os\n","\n","sys.path.insert(0, '/content/drive/My Drive/dop_notebooks')\n","#py_file_location = \"/content/drive/My Drive/dop_notebooks/pycache__\"\n","#sys.path.append(os.path.abspath(py_file_location))"],"metadata":{"id":"G7TxkQKwBN0V","executionInfo":{"status":"ok","timestamp":1648658161752,"user_tz":-330,"elapsed":551,"user":{"displayName":"ARSH TYAGI","userId":"08147742298405563713"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["!pip install pyproj==1.9.6"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mmpDPsdiq9qo","executionInfo":{"status":"ok","timestamp":1648657978619,"user_tz":-330,"elapsed":27408,"user":{"displayName":"ARSH TYAGI","userId":"08147742298405563713"}},"outputId":"07c96555-a025-4344-b8d5-9723faf745be"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting pyproj==1.9.6\n","  Downloading pyproj-1.9.6.tar.gz (2.8 MB)\n","\u001b[K     |████████████████████████████████| 2.8 MB 4.2 MB/s \n","\u001b[?25hBuilding wheels for collected packages: pyproj\n","  Building wheel for pyproj (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pyproj: filename=pyproj-1.9.6-cp37-cp37m-linux_x86_64.whl size=3702470 sha256=20c99125e9dd23c1ef89574dc1252f8b71b3341ad7453f0ae7dbaf2247d8cab0\n","  Stored in directory: /root/.cache/pip/wheels/08/60/55/38382877bab48a6abfca7bd66115e287ce5a9530cbce631771\n","Successfully built pyproj\n","Installing collected packages: pyproj\n","Successfully installed pyproj-1.9.6\n"]}]},{"cell_type":"code","source":["!apt install proj-bin libproj-dev libgeos-dev"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pp3e9DYMqwC6","executionInfo":{"status":"ok","timestamp":1648658021360,"user_tz":-330,"elapsed":7215,"user":{"displayName":"ARSH TYAGI","userId":"08147742298405563713"}},"outputId":"b4cde53a-665c-46e1-eee7-e244024b1d75"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Reading package lists... Done\n","Building dependency tree       \n","Reading state information... Done\n","libgeos-dev is already the newest version (3.6.2-1build2).\n","libproj-dev is already the newest version (4.9.3-2).\n","libproj-dev set to manually installed.\n","The following NEW packages will be installed:\n","  proj-bin\n","0 upgraded, 1 newly installed, 0 to remove and 39 not upgraded.\n","Need to get 32.3 kB of archives.\n","After this operation, 110 kB of additional disk space will be used.\n","Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 proj-bin amd64 4.9.3-2 [32.3 kB]\n","Fetched 32.3 kB in 1s (51.9 kB/s)\n","Selecting previously unselected package proj-bin.\n","(Reading database ... 156210 files and directories currently installed.)\n","Preparing to unpack .../proj-bin_4.9.3-2_amd64.deb ...\n","Unpacking proj-bin (4.9.3-2) ...\n","Setting up proj-bin (4.9.3-2) ...\n","Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n"]}]},{"cell_type":"code","source":["!pip install basemap"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":635},"id":"kI7wS5kFrQPi","executionInfo":{"status":"ok","timestamp":1648658120061,"user_tz":-330,"elapsed":15967,"user":{"displayName":"ARSH TYAGI","userId":"08147742298405563713"}},"outputId":"83aa80fa-afbc-4893-9983-5a95e1e51e7a"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting basemap\n","  Downloading basemap-1.3.2-cp37-cp37m-manylinux1_x86_64.whl (862 kB)\n","\u001b[?25l\r\u001b[K     |▍                               | 10 kB 15.2 MB/s eta 0:00:01\r\u001b[K     |▊                               | 20 kB 11.2 MB/s eta 0:00:01\r\u001b[K     |█▏                              | 30 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |█▌                              | 40 kB 3.7 MB/s eta 0:00:01\r\u001b[K     |██                              | 51 kB 3.7 MB/s eta 0:00:01\r\u001b[K     |██▎                             | 61 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |██▋                             | 71 kB 4.6 MB/s eta 0:00:01\r\u001b[K     |███                             | 81 kB 3.5 MB/s eta 0:00:01\r\u001b[K     |███▍                            | 92 kB 3.9 MB/s eta 0:00:01\r\u001b[K     |███▉                            | 102 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |████▏                           | 112 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |████▋                           | 122 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |█████                           | 133 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 143 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 153 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |██████                          | 163 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 174 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 184 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 194 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 204 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |████████                        | 215 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 225 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 235 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 245 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 256 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 266 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 276 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 286 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 296 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 307 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 317 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 327 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 337 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 348 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 358 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 368 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 378 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 389 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 399 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 409 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 419 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 430 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 440 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 450 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 460 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 471 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 481 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 491 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 501 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 512 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 522 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 532 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 542 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 552 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 563 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 573 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 583 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 593 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 604 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 614 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 624 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 634 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 645 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 655 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 665 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 675 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 686 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 696 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 706 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 716 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 727 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 737 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 747 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 757 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 768 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 778 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 788 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 798 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 808 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 819 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 829 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 839 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 849 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 860 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 862 kB 4.2 MB/s \n","\u001b[?25hRequirement already satisfied: six<1.16,>=1.10 in /usr/local/lib/python3.7/dist-packages (from basemap) (1.15.0)\n","Collecting basemap-data<1.4,>=1.3.2\n","  Downloading basemap_data-1.3.2-py2.py3-none-any.whl (30.5 MB)\n","\u001b[K     |████████████████████████████████| 30.5 MB 2.4 MB/s \n","\u001b[?25hRequirement already satisfied: numpy<1.23,>=1.21 in /usr/local/lib/python3.7/dist-packages (from basemap) (1.21.5)\n","Collecting pyshp<2.2,>=1.2\n","  Downloading pyshp-2.1.3.tar.gz (219 kB)\n","\u001b[K     |████████████████████████████████| 219 kB 46.7 MB/s \n","\u001b[?25hRequirement already satisfied: pyproj<3.4.0,>=1.9.3 in /usr/local/lib/python3.7/dist-packages (from basemap) (1.9.6)\n","Requirement already satisfied: matplotlib<3.6,>=1.5 in /usr/local/lib/python3.7/dist-packages (from basemap) (3.2.2)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib<3.6,>=1.5->basemap) (3.0.7)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib<3.6,>=1.5->basemap) (2.8.2)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib<3.6,>=1.5->basemap) (1.4.0)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib<3.6,>=1.5->basemap) (0.11.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib<3.6,>=1.5->basemap) (3.10.0.2)\n","Building wheels for collected packages: pyshp\n","  Building wheel for pyshp (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pyshp: filename=pyshp-2.1.3-py3-none-any.whl size=37324 sha256=80b2b284c2302071b8a14ddaf51d2766d9e018d22969649bb33ab49053bbd9cc\n","  Stored in directory: /root/.cache/pip/wheels/43/f8/87/53c8cd41545ba20e536ea29a8fcb5431b5f477ca50d5dffbbe\n","Successfully built pyshp\n","Installing collected packages: pyshp, basemap-data, basemap\n","  Attempting uninstall: pyshp\n","    Found existing installation: pyshp 2.2.0\n","    Uninstalling pyshp-2.2.0:\n","      Successfully uninstalled pyshp-2.2.0\n","Successfully installed basemap-1.3.2 basemap-data-1.3.2 pyshp-2.1.3\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["mpl_toolkits"]}}},"metadata":{}}]},{"cell_type":"code","source":["\n","import matplotlib\n","import numpy as np\n","from numpy import *\n","from array import array\n","\n","import math\n","import scipy.special\n","\n","import random\n","\n","import datetime\n","from osgeo import ogr, osr\n","\n","import BSTUtilities"],"metadata":{"id":"LvXHDjx9BDmw","executionInfo":{"status":"ok","timestamp":1648658166642,"user_tz":-330,"elapsed":581,"user":{"displayName":"ARSH TYAGI","userId":"08147742298405563713"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["def EMA_weights(N_events, N_Steps):\n","\n","    #   This method computes the weights for the Exponential Weighted Average (EMA)\n","\n","    alpha = 2./float((N_Steps+1))\n","\n","    #   time_series_list is the time series of floating point values\n","    #       arranged in order of first element in list being earliest\n","\n","    assert 0 < alpha <= 1\n","    \n","    weights = []\n","    \n","    #   Define the weights\n","    \n","    for i in range(0,N_events):\n","        weight_i = (1.0-alpha)**i\n","        weights.append(weight_i)\n","        \n","    sum_weights = sum(weights)\n","    weights =  [i/sum_weights for i in weights]\n","     \n","    return weights\n","    \n","    #.................................................................\n","    \n","def EMA_weighted_time_series(time_series, N_Steps):\n","\n","    #   This method computes the Exponential Weighted Average of a list.  Last\n","    #       in the list elements are exponentially weighted the most\n","\n","    N_events = len(time_series)\n","    \n","    weights = EMA_weights(N_events, N_Steps)\n","    \n","    weights_reversed = list(reversed(weights))\n","\n","    \n","#     print 'N_events: ', N_events\n","#     print ''\n","#     print 'weights_reversed: ', weights_reversed\n","#     print ''\n","#     print 'time_series: ', time_series\n","#     print ''\n","    \n","#     print ''\n","#     print time_series\n","#     print ''\n","#     print weights_reversed\n","#     print ''\n","\n","    EMA_weighted_ts = []\n","    partial_weight_sum = 0.\n","    \n","    for i in range(N_events):\n","        partial_weight_sum += weights[i]\n","        weighted_ts = round(float(time_series[i])*weights_reversed[i],4)\n","        \n","        EMA_weighted_ts.append(weighted_ts)\n","        \n","    partial_weight_sum = round(partial_weight_sum,4)\n","    sum_value = sum(EMA_weighted_ts)\n","    \n","    if (float(partial_weight_sum)) <= 0.0:\n","        sum_value = 0.0001\n","        partial_weight_sum = 1.\n","    \n","    weighted_sum = float(sum_value)/float(partial_weight_sum)\n","    \n","    return weighted_sum\n","    \n","    #................................................................."],"metadata":{"id":"Ojo5WmpVBJx4","executionInfo":{"status":"ok","timestamp":1648658172091,"user_tz":-330,"elapsed":877,"user":{"displayName":"ARSH TYAGI","userId":"08147742298405563713"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["def build_daily_counts(burst_min_size, min_daily_events, mag, lat, lng, date, time, yrs, sd_factor, burst_print_flag):\n","\n","    #   First find lat and lng of burst centroid.  Then find standard deviation.\n","    #\n","    \n","    max_duration = 365.0*float((max(yrs) - min(yrs) ))    # In days\n","    \n","    number_events = [0. for i in range(int(max_duration))]\n","    index_events  = [[] for i in range(int(max_duration))]\n","    \n","    for i in range(len(yrs)):\n","        \n","        day_of_event = 365.0*float(yrs[i] - min(yrs))\n","\n","        k = int(day_of_event)-1     #   Index of the day\n","        \n","        number_events[k] += 1      #   Tabulates number of events on each day\n","        \n","        index_events[k].append(i)\n","        \n","    burst_index = combine_daily_count_bursts(min_daily_events, number_events, index_events)\n","    \n","    #   Clean the data by removing outliers\n","    \n","    burst_index =   clean_data_outliers(burst_index, lat, lng, burst_min_size, sd_factor)\n","    \n","    #   ---------------------------------------------------\n","    \n","    basic_event_rate = float(len(yrs))/(float(max(yrs)-min(yrs))*365.0)\n","    \n","    burst_number = 0\n","    \n","    total_number_events_in_bursts = 0\n","    number_events_in_large_bursts = 0\n","    \n","#     for i in range(len(burst_index)):\n","#         print 'i, burst_index: ', i, burst_index[i]\n","\n","    save_bursts = 'ON'\n","\n","    if save_bursts == 'ON':\n","        output_file_name = 'burst_file.txt'\n","        burst_file = open('burst_file.txt', 'w')\n","    \n","    for i in range(len(burst_index)):\n","    \n","        total_number_events_in_bursts += int(len(burst_index[i]))\n","    \n","        lat_list = []\n","        lng_list = []\n","            \n","        for j in range(len(burst_index[i])):\n","        \n","            k = burst_index[i][j]\n","            lat_list.append(lat[k])\n","            lng_list.append(lng[k])\n","        \n","        burst_centroid_lat = BSTUtilities.mean_val(lat_list)\n","        burst_centroid_lng = BSTUtilities.mean_val(lng_list)\n","        \n","        std_dev_lat, var_lat = BSTUtilities.std_var_val(lat_list)\n","        std_dev_lng, var_lng = BSTUtilities.std_var_val(lng_list)\n","        \n","        first_index = burst_index[i][0]\n","        k = len(burst_index[i])-1\n","        second_index = burst_index[i][k]\n","        \n","        start_date = date[first_index]\n","        end_date   = date[second_index]\n","        \n","        start_time = time[first_index]\n","        end_time   = time[second_index]\n","            \n","        number_events_burst = int(len(burst_index[i]))\n","        \n","        burst_duration =  int( 365.0*float(yrs[second_index] - yrs[first_index]) ) + 1\n","        \n","        \n","        if start_date != end_date:\n","            burst_duration += 1\n","   \n","        duration_rate = float(number_events_burst)/float(burst_duration)\n","        \n","        if len(burst_index[i]) >= burst_min_size and burst_print_flag == 'ON':\n","        \n","            number_events_in_large_bursts += int(len(burst_index[i]))\n","        \n","            print('')\n","            print('Burst Number:', i, 'Number of Events in Swarm:', len(burst_index[i]))\n","            print('')\n","            print('Start Date & Time: ', start_date,' @ ',start_time)\n","            print('')\n","            print('  End Date & Time: ', end_date, ' @ ', end_time)\n","            print('')\n","            print('Swarm centered at (degrees in lat,lng): ', round(burst_centroid_lat,4), round(burst_centroid_lng,4))\n","            print('')\n","            print('Swarm size (standard deviation in degrees lat,lng): ', round(std_dev_lat,4), round(std_dev_lng,4))\n","            print('')\n","            print('Swarm duration (days): ', int(burst_duration))\n","            print('')\n","            print('Basic rate, Duration rate (per day): ', round(basic_event_rate,4), round(duration_rate,4))\n","            print('')\n","            print('Minimum Burst Size in Plots: ', burst_min_size)\n","            print('')\n","            print('----------------------------------------')\n","            \n","            if save_bursts == 'ON':\n","                print('', file=burst_file)\n","                print('Burst Number:', i, 'Number of Events in Swarm:', len(burst_index[i]), file=burst_file)\n","                print('', file=burst_file)\n","                print('Start Date & Time: ', start_date,' @ ',start_time, file=burst_file)\n","                print('', file=burst_file)\n","                print('  End Date & Time: ', end_date, ' @ ', end_time, file=burst_file)\n","                print('', file=burst_file)\n","                print('Swarm centered at (degrees in lat,lng): ', round(burst_centroid_lat,4), round(burst_centroid_lng,4), file=burst_file)\n","                print('', file=burst_file)\n","                print('Swarm size (standard deviation in degrees lat,lng): ', round(std_dev_lat,4), round(std_dev_lng,4), file=burst_file)\n","                print('', file=burst_file)\n","                print('Swarm duration (days): ', int(burst_duration), file=burst_file)\n","                print('', file=burst_file)\n","                print('Basic rate, Duration rate (per day): ', round(basic_event_rate,4), round(duration_rate,4), file=burst_file)\n","                print('', file=burst_file)\n","                print('Minimum Burst Size in Plots: ', burst_min_size, file=burst_file)\n","                print('', file=burst_file)\n","                print('----------------------------------------', file=burst_file)\n","        \n","        burst_number += 1\n","        \n","    print('----------------------------------------')\n","    print('')\n","    print('Number of Bursts for the Entire USGS Circle Catalog: ', len(burst_index))\n","    print('')\n","    print('Percent of Events Contained in Bursts of Any Size: ', str(round(100.0 * \\\n","            float(total_number_events_in_bursts)/float(len(yrs)),2))+'%')\n","#     print ''\n","#     print 'Percent of Events Contained in Large Bursts: ', str(round(100.0 * float(number_events_in_large_bursts)/float(len(yrs)),2))+'%'\n","    print('')\n","    print('----------------------------------------')\n","    print('----------------------------------------')\n","\n","    print('')\n","    \n","    if save_bursts == 'OFF':\n","        burst_file.close()\n","    \n","    #   ---------------------------------------------------\n","    \n","    return number_events, index_events, burst_index\n","        \n","    #   ---------------------------------------------------\n","    \n","def combine_daily_count_bursts(min_daily_events, number_events, index_events):\n","\n","    #   This method combines the daily counts into coherent burst event swarms\n","    \n","    temp_burst_index    =   []\n","    burst_index_prelim  =   []\n","    burst_index         =   []\n","    \n","    for i in range(len(number_events)):\n","         #   This is a burst\n","        if i == 0 and number_events[0] >= min_daily_events:    \n","            for j in range(len(index_events[0])):\n","                temp_burst_index.append(index_events[0][j])\n","            \n","        # Allows possibility of including foreshocks\n","        elif i > 0 and i< len(number_events)-1 and number_events[i] < min_daily_events and number_events[i+1] >= min_daily_events:\n","            for j in range(len(index_events[i])):\n","                temp_burst_index.append(index_events[i][j])\n","            \n","        #   Start a new burst\n","        elif i > 0 and number_events[i] >= min_daily_events and number_events[i-1] < min_daily_events: #   Add this to burst\n","            for j in range(len(index_events[i])):\n","                temp_burst_index.append(index_events[i][j])\n","        \n","        #   Add this to the current burst\n","        elif i > 0 and number_events[i] >= min_daily_events and number_events[i-1] >= min_daily_events: #   Add this to burst\n","            for j in range(len(index_events[i])):\n","                temp_burst_index.append(index_events[i][j])\n","\n","        #   Terminate the current burst and reset the temp_burst_index\n","        elif i > 0 and number_events[i] < min_daily_events and number_events[i-1] >= min_daily_events and len(temp_burst_index) > 0:\n","            burst_index_prelim.append(temp_burst_index)\n","            temp_burst_index    =   []\n","            \n","        else:\n","            pass\n","    \n","    # Ensure that the number of events in the burst is at least the required minimum\n","    \n","    for i in range(len(burst_index_prelim)):\n","        if len(burst_index_prelim[i]) >= min_daily_events:\n","            burst_index.append(burst_index_prelim[i])\n","            \n","    return  burst_index\n","    \n","    #................................................................."],"metadata":{"id":"abnvBuFiB-bP","executionInfo":{"status":"ok","timestamp":1648658176650,"user_tz":-330,"elapsed":1012,"user":{"displayName":"ARSH TYAGI","userId":"08147742298405563713"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["def reject_outliers_earthquakes(index_list, data_lat, data_lng, sd_factor):\n","\n","    #   This method removes lat-lng points that are far from the median of the burst\n","    \n","    median_lat = np.median(data_lat)\n","    median_lng = np.median(data_lng)\n","    \n","    great_circle_distance   =   []\n","    data_lat_removed        =   []\n","    data_lng_removed        =   []\n","    index_list_removed      =   []\n","    \n","    for i in range(len(data_lat)):\n","        lat1 = median_lat\n","        lng1 = median_lng\n","        lat2 = data_lat[i]\n","        lng2 = data_lng[i]\n","        dist = BSTUtilities.compute_great_circle_distance(lat1, lng1, lat2, lng2)\n","        great_circle_distance.append(dist)\n","        \n","    median_gcdist = np.median(great_circle_distance)\n","    \n","    for i in range(len(data_lat)):\n","        if great_circle_distance[i] < sd_factor * median_gcdist:\n","            data_lat_removed.append(data_lat[i])\n","            data_lng_removed.append(data_lng[i])\n","            index_list_removed.append(index_list[i])\n","            \n","#     index_list_removed = index_list\n","            \n","    return index_list_removed\n","    \n","    #.................................................................\n","    \n","def clean_data_outliers(burst_index_test, lat, lng, burst_min_size, sd_factor):\n","\n","    burst_index    =   []\n","        \n","    for i in range(len(burst_index_test)):\n","        index_list = burst_index_test[i]\n","        lat_list = []\n","        lng_list = []\n","            \n","        for j in range(len(index_list)):\n","            lat_list.append(lat[index_list[j]])\n","            lng_list.append(lng[index_list[j]])\n","    \n","        index_list_removed = reject_outliers_earthquakes(index_list, lat_list, lng_list, sd_factor)\n","        \n","        if len(index_list_removed) >= burst_min_size:\n","            burst_index.append(index_list_removed)\n","        \n","    #   Make sure all bursts are non-empty\n","            \n","    return burst_index\n","    \n","    #.................................................................\n","    \n","def compute_burst_radius_gyration(index_list, mag, lat, lng, date, time, years):\n","    \n","    lat_list = []\n","    lng_list = []\n","    mag_list = []\n","    \n","    great_circle_distance_2 =   []\n","    \n","    for i in range(len(index_list)):\n","\n","        lat_list.append(lat[index_list[i]])\n","        lng_list.append(lng[index_list[i]])\n","        mag_list.append(mag[index_list[i]])\n","        \n","        burst_centroid_lat = BSTUtilities.mean_val(lat_list)\n","        burst_centroid_lng = BSTUtilities.mean_val(lng_list)\n","        \n","    #   Compute Radius of Gyration\n","    \n","    for i in range(len(lat_list)):\n","\n","        lat1 = burst_centroid_lat\n","        lng1 = burst_centroid_lng\n","        lat2 = lat_list[i]\n","        lng2 = lng_list[i]\n","        dist = BSTUtilities.compute_great_circle_distance(lat1, lng1, lat2, lng2)\n","            \n","        great_circle_distance_2.append(math.pow(dist,2))\n","        \n","    radius_gyration = math.pow(BSTUtilities.mean_val(great_circle_distance_2), 0.5)\n","    \n","    return radius_gyration, burst_centroid_lat, burst_centroid_lng\n","\n","    #................................................................."],"metadata":{"id":"8b65zuQhCLoI","executionInfo":{"status":"ok","timestamp":1648658181878,"user_tz":-330,"elapsed":648,"user":{"displayName":"ARSH TYAGI","userId":"08147742298405563713"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["def reject_outliers_clusters(rgyr_cluster, year_cluster, sd_cluster):\n","\n","    #   This method removes lat-lng points that are far from the median of the burst\n","    \n","    rgyr_cluster_filtered   =   []\n","    year_cluster_filtered   =   []\n","    \n","    median_rgyr = np.median(rgyr_cluster)\n","    \n","    print('')\n","    print('----------------------------------------')\n","    print('')\n","    print('Median Radius of Gyration: ', str(round(median_rgyr,2)) + ' (km)')\n","    print('')\n","\n","\n","    \n","    number_all_clusters = len(rgyr_cluster)\n","\n","    for i in range(len(rgyr_cluster)):\n","    \n","        if rgyr_cluster[i] < sd_cluster * median_rgyr:\n","            rgyr_cluster_filtered.append(rgyr_cluster[i])\n","            year_cluster_filtered.append(year_cluster[i])\n","            \n","    number_reduced_clusters = len(rgyr_cluster_filtered)\n","    \n","    print('Fraction of Clusters Removed: ', str(100.0*(1 - float(number_reduced_clusters)/float(number_all_clusters))) + '%')\n","    print('')\n","    print('----------------------------------------')\n","    print('')\n","\n","    return rgyr_cluster_filtered, year_cluster_filtered\n","    \n","    #.................................................................\n","    \n","def reject_outliers_clusters_alternate(ratio_limit, burst_index, burst_min_size, mag, lat, lng, date, time, years):\n","\n","    #   This method removes clusters that are not compact\n","    \n","    mass_list           =   []\n","    year_list           =   []\n","    radgyr_list         =   []\n","    index_cluster       =   []\n","    centroid_lat_list   =   []\n","    centroid_lng_list   =   []\n","    \n","#     for i in range(3,len(burst_index)):        #   Iterating over the bursts/swarms\n","    for i in range(len(burst_index)):        #   Iterating over the bursts/swarms\n","    \n","        if len(burst_index[i]) >= burst_min_size:\n","        \n","            index_list = burst_index[i]\n","            lat_list    = []\n","            lng_list    = []\n","\n","            \n","            mass_list.append(float(len(burst_index[i])))    #   Number of events in the burst\n","\n","            year_list.append(years[burst_index[i][0]])      #   Year of the first event in the burst\n","            \n","            for j in range(len(burst_index[i])):\n","            \n","                lat_list.append(lat[index_list[j]])\n","                lng_list.append(lng[index_list[j]])\n","\n","            radius_gyration, burst_centroid_lat, burst_centroid_lng = \\\n","                    compute_burst_radius_gyration(burst_index[i], mag, lat, lng, date, time, years)\n","            \n","            radgyr_list.append(radius_gyration)     #   This is the time series we want to use EMA on\n","            \n","            index_cluster.append(index_list[0])\n","            \n","            centroid_lat_list.append(burst_centroid_lat)\n","            centroid_lng_list.append(burst_centroid_lng)\n","            \n","    density_list   =   []\n","    \n","    for i in range(len(mass_list)):\n","        density = mass_list[i]/((radgyr_list[i]))\n","        density_list.append(density)\n","        \n","    density_list = [math.log(density_list[i],10) for i in range(len(density_list))]\n","    \n","    rgyr_cluster_filtered   =   []\n","    year_cluster_filtered   =   []\n","    index_cluster_filtered  =   []\n","    centroid_lat_filtered   =   []\n","    centroid_lng_filtered   =   []\n","    \n","    for i in range(len(mass_list)):\n","        if density_list[i] >= ratio_limit:\n","            rgyr_cluster_filtered.append(radgyr_list[i])\n","            year_cluster_filtered.append(year_list[i])\n","            index_cluster_filtered.append(index_cluster[i])\n","            centroid_lat_filtered.append(centroid_lat_list[i])\n","            centroid_lng_filtered.append(centroid_lng_list[i])\n","        \n","    return rgyr_cluster_filtered, year_cluster_filtered, index_cluster_filtered, centroid_lat_filtered, centroid_lng_filtered\n","    \n","    #.................................................................\n","    \n","def calc_radgyr_EMA_time_dev(burst_min_size, completeness_mag, Location, N_Steps,\\\n","        circle_catalog_date_start, circle_catalog_date_end, burst_index, mag, lat, lng, date, time, years, \\\n","        ratio_limit, sd_factor, cutoff_start_year, regional_rate):\n","        \n","    #   This routine calculates a single radius of gyration time series for given ENF and CLF values\n","    \n","    mean_lat        =   []\n","    mean_lng        =   []\n","    mean_yrs        =   []\n","    swarm_distance  =   []\n","    year_list       =   []\n","    radgyr_list     =   []\n","    radgyr_EMA_list =   []\n","    year_swarm      =   []\n","    index_swarm     =   []\n","    \n","\n","    for i in range(3,len(burst_index)):        #   Iterating over the bursts/swarms\n","    \n","        if len(burst_index[i]) >= burst_min_size:\n","        \n","            index_list = burst_index[i]\n","            lat_list = []\n","            lng_list = []\n","            yrs_list = []\n","                \n","            for j in range(len(burst_index[i])):\n","            \n","                lat_list.append(lat[index_list[j]])\n","                lng_list.append(lng[index_list[j]])\n","                yrs_list.append(years[index_list[j]])\n","                \n","            radius_gyration, burst_centroid_lat, burst_centroid_lng = \\\n","                    compute_burst_radius_gyration(burst_index[i], mag, lat, lng, date, time, years)\n","            \n","            radgyr_list.append(radius_gyration)     #   This is the time series we want to use EMA on\n","            year_swarm.append(years[index_list[0]])\n","            index_swarm.append(index_list[0])\n","\n","#   Filter the clusters now, we filtered the events in the clusters previously\n","    \n","    radgyr_list, year_swarm, index_swarm, centroid_lat, centroid_lng = \\\n","            reject_outliers_clusters_alternate(ratio_limit, burst_index, burst_min_size, mag, lat, lng, date, time, years)\n","\n","    #   Independent of any start date down to here -------------------------\n","            \n","    average_rate_start_date =   circle_catalog_date_start\n","    average_rate_end_date   =   circle_catalog_date_end\n","    \n","    if average_rate_end_date < 1990.0:\n","        average_rate_end_date = 1990.0      #   The beginning of decent digital data\n","    \n","    year_interval = average_rate_end_date - average_rate_start_date\n","    \n","    number_earthquakes = 0\n","    number_bursts_in_plot = 0\n","    \n","    for i in range(len(burst_index)):\n","        if years[burst_index[i][0]] >= average_rate_start_date:\n","            number_earthquakes += 1\n","            number_bursts_in_plot += 1\n","        \n","    for i in range(1,len(radgyr_list)+1):\n","        RG_list_raw = []\n","        \n","        for j in range(i):\n","            RG_list_raw.append(radgyr_list[j])\n","            \n","        radgyr_EMA = EMA_weighted_time_series(RG_list_raw, N_Steps)\n","        \n","        radgyr_EMA_list.append(radgyr_EMA)\n","        \n","    #   Determine which bursts occurred after 1990.0\n","    \n","#     kk = 0\n","    \n","    year_swarm_reduced  =   []\n","    radgyr_EMA_reduced  =   []\n","    index_swarm_reduced  =   []\n","    \n","    for i in range(len(year_swarm)):\n","        if year_swarm[i] > cutoff_start_year:\n","            year_swarm_reduced.append(year_swarm[i])\n","            radgyr_EMA_reduced.append(radgyr_EMA_list[i])\n","            index_swarm_reduced.append(index_swarm[i])\n","            \n","    year_swarm = year_swarm_reduced\n","    radgyr_EMA_list = radgyr_EMA_reduced\n","    \n","    print('')\n","    print('----------------------------------------------------------')\n","    print('')\n","    print('ENF, CLF, len(year_swarm), len(radgyr_EMA_list): ', ratio_limit, sd_factor, len(year_swarm), len(radgyr_EMA_list))\n","    \n","    \n","#   -----------------------------------------------------------------\n","#   Here we build a continuous, constant length, regular interval, \n","#       time series for radgyr as a function of time\n","#\n","#   We are basically interpolating between points here\n","#\n","#     radgyr_EMA_list = [round(i,2) for i in radgyr_EMA_list]\n","#     print 'radgyr_EMA_list: ', radgyr_EMA_list\n","# #     \n","        \n","    for i in range(len(years)):\n","        if years[i] <= cutoff_start_year:       #   Find the index corresponding to cutoff_year\n","            cutoff_index = i\n","        else:\n","            pass\n","    \n","    length_lists = len(years) - cutoff_index\n","    \n","    year_swarm_regular  =   []\n","    radgyr_list_regular =   []\n","    \n","    for i in range(length_lists):\n","#         print 'len(years), i+length_lists: ', len(years), i+cutoff_index\n","        year_swarm_regular.append(years[i+cutoff_index])\n","        radgyr_list_regular.append(0.0)\n","        \n","    for i in range(len(radgyr_EMA_list)):\n","        radgyr_list_regular[index_swarm_reduced[i]-cutoff_index] = radgyr_EMA_list[i]\n","    \n","    radgyr_value = radgyr_EMA_list[0]\n","    \n","    for i in range(len(radgyr_list_regular)):\n","\n","        if radgyr_list_regular[i] == 0.0:\n","            radgyr_list_regular[i] += round(radgyr_value,2)\n","        \n","        if radgyr_list_regular[i] > 0.0:\n","            radgyr_value = round(radgyr_list_regular[i],2)\n","            \n","\n","#     print('Length of year_swarm_regular time series', len(year_swarm_regular))\n","#   Here we write the values of the interpolated radii of gyration into a file.\n","#       Values are 1 week prior to the major earthquakes.  We will use this\n","#       data to optimize the weights for the time series\n","\n","    write_precursor_radii_to_file(years, mag, year_swarm_regular,radgyr_list_regular, ratio_limit, sd_factor)\n","\n","    return year_swarm_regular,radgyr_list_regular\n","    \n","    #................................................................."],"metadata":{"id":"nqzFvooMCT6v","executionInfo":{"status":"ok","timestamp":1648658186643,"user_tz":-330,"elapsed":943,"user":{"displayName":"ARSH TYAGI","userId":"08147742298405563713"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["def write_precursor_radii_to_file(years, mag, year_swarm_regular,radgyr_list_regular, ratio_limit, sd_factor):\n","\n","#   ratio_limit is ENF.  sd_factor is CLF\n","\n","    interpolation_file_name = 'interpolation_file.txt'\n","    interpolation_file = open(interpolation_file_name, 'a')\n","    \n","#   Find dates of large earthquakes\n","\n","    earthquake_year_M7  =   []\n","    \n","    for i in range(len(years)):\n","        if float(mag[i]) >= 7.0:\n","            earthquake_year_M7.append(years[i])\n","\n","    radgyr_M7_EQ    = [0.0 for kk in range(4)]\n","    year_M7_EQ      = [0.0 for kk in range(4)]\n","    \n","    kk = -1\n","\n","    for ii in range(4):\n","        \n","            for jj in range(len(year_swarm_regular)):\n","                if year_swarm_regular[jj] < earthquake_year_M7[ii]-0.0192:     #  Use date of M>7 earthquake minus 1 week since error in\n","                                                                        #    year-to-day conversion accumulates over time\n","                    year_M7_EQ[ii]      = year_swarm_regular[jj]        #  Overwrite the data until the large EQ date is passed\n","                    radgyr_M7_EQ[ii]    = radgyr_list_regular[jj]\n","\n","    data_list = ratio_limit, sd_factor, year_M7_EQ[0], year_M7_EQ[1],year_M7_EQ[2],year_M7_EQ[3], \\\n","            radgyr_M7_EQ[0], radgyr_M7_EQ[1],radgyr_M7_EQ[2],radgyr_M7_EQ[3]\n","    print(ratio_limit, sd_factor, year_M7_EQ[0], year_M7_EQ[1],year_M7_EQ[2],year_M7_EQ[3], \\\n","            radgyr_M7_EQ[0], radgyr_M7_EQ[1],radgyr_M7_EQ[2],radgyr_M7_EQ[3], file=interpolation_file)    \n","\n","    interpolation_file.close()\n","    \n","    return\n","\n","    #.................................................................\n","    \n","def weight_optimizer():\n","\n","#   This code optimizes the combination of the radii of gyration curves using \n","#       a brute force optimization\n","\n","    interpolation_file_name = 'interpolation_file.txt'\n","    interpolation_file = open(interpolation_file_name, 'r')\n","    \n","    count = len(open(interpolation_file_name).readlines(  ))\n","    print('')\n","    print('length of interpolation file: ',count)\n","    print('')\n","    \n","    ENF_list    =   []\n","    CLF_list    =   []\n","\n","    radii_list  =   [[] for i in range(count)]  #   A list of lists\n","    \n","    i=0\n","    for line in interpolation_file:\n","\n","        items = line.strip().split()\n","#         print(items)\n","#         print(items[6],items[7],items[8],items[9])\n","        \n","        ENF_list.append(items[0])\n","        CLF_list.append(items[1])\n","        \n","        radii_list[i].extend((items[6],items[7],items[8],items[9]))\n","        \n","        i += 1\n","        \n","#     for i in range(len(radii_list)):\n","#         print('radii_list: ', radii_list[i])\n","        \n","    #   print initial non-normalized weights\n","\n","    initial_weights =   [1.0/float(count) for i in range(count)]     #   We start with equal weighting\n","    \n","    adjusted_weights=   [1.0/float(count) for i in range(count)]\n","#     \n","# #     print('')\n","# #     print('Initial Non-Normalized Weights: ', initial_weights)\n","# #     print('')\n","#     \n","# #     l = [random.randint(0,1) for i in range(100)]\n","# #     print('l: ',l)\n","# #     \n","#     \n","#     for kk in range(len(ENF_list)):\n","#     \n","#         random_weights = random.sample(range(0, len(initial_weights)), len(initial_weights))\n","#         sum_of_weights = sum(random_weights)\n","#         random_weights = [float(random_weights[i])/sum_of_weights for i in range(len(random_weights))]\n","    \n","    mean_radius_4 = [0. for i in range(4)]\n","    \n","    for i in range(4):\n","        for j in range(count):\n","#             mean_radius[i] += random_weights[j]*float(radii_list[j][i])\n","            mean_radius_4[i] += initial_weights[j]*float(radii_list[j][i])\n","    \n","    mean_radius_init = np.mean(mean_radius_4)\n","    mean_radius_init = round(mean_radius_init,2)\n","    \n","    std_radius_init  = np.std(mean_radius_4)\n","    std_radius_init = round(std_radius_init,2)\n","    \n","    mean_radius_4_init = [round(mean_radius_4[i],2) for i in range(4)]\n","    \n","    trials = 20000\n","    \n","    min_mean_radius = mean_radius_init\n","    min_std_radius  = std_radius_init\n","    \n","    for k in range(trials):       \n","    \n","        random_weights = random.sample(range(0, count), count)\n","        sum_of_weights = sum(random_weights)\n","        random_weights = [float(random_weights[i])/sum_of_weights for i in range(len(random_weights))]\n","        \n","        mean_radius_4 = [0. for i in range(4)]\n","        \n","        for i in range(4):\n","\n","            for j in range(count):\n","                mean_radius_4[i] += random_weights[j]*float(radii_list[j][i])\n","    \n","        mean_radius = np.mean(mean_radius_4)\n","        mean_radius = round(mean_radius,2)\n","    \n","        std_radius  = np.std(mean_radius_4)\n","        std_radius = round(std_radius,2)\n","        \n","        mean_radius_4 = [round(mean_radius_4[i],2) for i in range(4)]\n","#         print('Adjusted Mean_Radii (Km): ', mean_radius_4)\n","#         print('Mean Adjusted Radius Value: ', mean_radius, '+/-', std_radius, 'Km)')\n","#         print('')\n","        \n","        if (std_radius < min_std_radius):\n","            min_std_radius = std_radius\n","            min_mean_radius = mean_radius\n","            min_random_weights = random_weights\n","            \n","#     print(min_random_weights)\n","\n","\n","        \n","    print('')\n","    print('------------------------------------------------')\n","    print('')\n","    print('Non-adjusted Mean_Radii (Km): ', mean_radius_4_init)\n","    print('')\n","    print('Mean Non-Adjusted Radius Value: ', mean_radius_init, '+/-', std_radius_init, 'Km')\n","    print('')\n","    print('Minimum Mean Adjusted Radius Value: ', min_mean_radius, '+/-', min_std_radius, 'Km')\n","    print('')\n","    \n","    for i in range(4):\n","        values_list = []\n","        \n","        for j in range(count):\n","            values_list.append(float(radii_list[j][i]))\n","        \n","        mean_ts, std_ts = time_series_mean_std(values_list, min_random_weights)\n","        \n","        print('EQ: ',i+1,' Minimum Mean Adjusted Radius Value: ', round(mean_ts,2), '+/-', round(std_ts,2), 'Km')\n","        print('')\n","    \n","    print('------------------------------------------------')\n","    print('')\n","\n","    \n","    interpolation_file.close()\n","\n","#     return initial_weights and adjusted weights\n","\n","    return initial_weights, adjusted_weights\n","    #................................................................."],"metadata":{"id":"jQYTwOzOG1qd","executionInfo":{"status":"ok","timestamp":1648658191475,"user_tz":-330,"elapsed":1083,"user":{"displayName":"ARSH TYAGI","userId":"08147742298405563713"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["def time_series_mean_std(values_list, weight_list):\n","\n","#   This method computes the mean and standard deviation of the time series\n","#       at every time step using the adjusted time series weights\n","\n","    mean_ts = 0.\n","    std_ts = 0.\n","    \n","    mean_ts_list = []\n","    \n","    for i in range(len(values_list)):\n","    \n","        mean_term = float(values_list[i])*float(weight_list[i])\n","        mean_ts_list.append( mean_term )\n","        \n","    mean_ts = sum(mean_ts_list)\n","    \n","    variance_ts_list = []\n","    \n","    for i in range(len(values_list)):\n","    \n","        variance_term = float(weight_list[i]) * math.pow(float(values_list[i])-float(mean_ts),2)\n","        variance_ts_list.append( variance_term )\n","            \n","    std_ts = math.pow(sum(variance_ts_list),0.5)\n","\n","    return mean_ts, std_ts\n","    \n","    #................................................................."],"metadata":{"id":"PejqV6CTG_BF","executionInfo":{"status":"ok","timestamp":1648658193824,"user_tz":-330,"elapsed":4,"user":{"displayName":"ARSH TYAGI","userId":"08147742298405563713"}}},"execution_count":9,"outputs":[]}]}